# The Forbidden Code - Python Examples

This repository contains Python code samples chapter-wise from the book *The Forbidden Code*.

## Structure
- Chapter 1: The Seeds of Creation – Data as the Fruit of Eden (['import pandas as pd', '# Load sample dataset', 'df = pd.DataFrame(data)', '# Calculate approval rates by gender', 'approval_rates = df.groupby("gender")["loan_approved"].mean()'] examples)
- Chapter 2: The Temptation of Speed – Shipping Without Ethics (['import matplotlib.pyplot as plt', '# Define risks with (likelihood, impact)', '# Plot risk points', 'plt.figure(figsize=(8, 6))', 'plt.scatter(likelihood, impact, s=100, label=risk)', '# Add labels and grid', 'plt.legend(loc="upper left", bbox_to_anchor=(1, 1))'] examples)
- Chapter 3: Privacy in Paradise – Guarding the Garden (['# Example: Simulating aggregation pitfalls', 'import pandas as pd', '# Sample fitness data', '# Aggregating into a heat map simulation', "heat_map = fitness_data.groupby('location').size()", '# Example: Demonstrating principle of least privilege', 'class CameraSystem:', 'def __init__(self):', 'def get_feed(self, user_role):', '# Should require warrant and limited access', '# Simulating', 'system = CameraSystem()', '# Example: Implementing consent in code', 'class DataCollector:', 'def __init__(self):', 'def collect(self, user_input, consent):', '# Simulating', 'collector = DataCollector()', 'print(collector.collect("User Location", consent=False))', 'print(collector.collect("User Location", consent=True))'] examples)
- Chapter 4: Whispers in the Syntax – How Bias Enters Code (['from gensim.models import KeyedVectors', "# Load Google's pre-trained Word2Vec embeddings", '# (for illustration only; in practice use local download)', 'model = KeyedVectors.load_word2vec_format(model_path, binary=True)', '# Test analogy: man is to computer programmer as woman is to ?', 'result = model.most_similar(positive=["woman", "programmer"], negative=["man"], topn=5)', 'import numpy as np', 'def debias_vector(word, gender_direction):', 'projection = np.dot(vector, gender_direction) /', '# Compute a simple gender direction', '# Debias selected words', 'debiased_vectors = {w: debias_vector(w, gender_direction) for w in words_to_debias}', '# Check cosine similarity before and after', 'from numpy.linalg import norm', 'def cosine_similarity(vec1, vec2):'] examples)
- Chapter 5: False Prophets – When Algorithms Pretend to Be Fair (['# Import necessary libraries', 'import shap', 'import xgboost as xgb', 'from sklearn.datasets import load_breast_cancer', 'from sklearn.model_selection import train_test_split', 'from sklearn.metrics import accuracy_score', '# Load dataset', 'data = load_breast_cancer()', '# Split into training and testing', 'X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)', '# Train a black-box model (XGBoost)', "model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss')", '# Make predictions', 'y_pred = model.predict(X_test)', '# Initialize SHAP explainer', 'explainer = shap.Explainer(model, X_train)', 'shap_values = explainer(X_test)', '# Visualize explanation for one prediction'] examples)
- Chapter 6: The Mirror of Man – AI Reflecting Our Flaws (['import pandas as pd', 'from sklearn.metrics import confusion_matrix', '# Sample dataset with demographic bias', 'df = pd.DataFrame(data)', 'def group_accuracy(df, group_col, outcome_col, pred_col):', 'groups = df[group_col].unique()', 'cm = confusion_matrix(subset[outcome_col], subset[pred_col])', 'accuracy = (cm[0,0] + cm[1,1]) / cm.sum()', 'accuracy_by_gender = group_accuracy(df, "gender", "approved", "prediction")'] examples)
- Chapter 7: The Fall into the Wild – Deploying Without Watchtowers (['from sklearn.ensemble import', 'from sklearn.model_selection import', 'from sklearn.metrics import accuracy_score', 'import pandas as pd', '# Load dataset', 'data = pd.read_csv("transactions.csv")', 'X = data.drop("fraud", axis=1)', '# Split dataset', 'train_test_split(X, y, test_size=0.2)', '# Train model', 'model = RandomForestClassifier()', '# Evaluate on test set', 'predictions = model.predict(X_test)', 'import numpy as np', 'from sklearn.metrics import confusion_matrix', 'def demographic_parity(y_true, y_pred,', '# Calculate positive prediction rate per group', 'groups = np.unique(sensitive_feature)', 'rates[group] = np.mean(y_pred[mask])', '# Example usage', 'y_true = np.array([0, 1, 0, 1, 1, 0])', 'y_pred = np.array([0, 1, 0, 0, 1, 1])', 'import logging', 'from datetime import datetime', 'format="%(asctime)s - %(levelname)s - %(message)s"', 'def predict_and_log(model, input_data):', 'prediction = model.predict(input_data)', 'import pandas as pd', 'from sklearn.metrics import confusion_matrix', 'def fairness_audit(y_true, y_pred, sensitive_attr):', 'df = pd.DataFrame({"y_true": y_true, "y_pred": y_pred, "group": sensitive_attr})', 'tn, fp, fn, tp = confusion_matrix(y_t, y_p).ravel()', 'results[g] = {"False_Positive_Rate": fp / (fp + tn)}'] examples)
- Chapter 8: The Wrath of the World – When AI Hurts People (['def deploy_ai_model(model, safeguards, testing_hours):', 'class EthicalAutopsy:', 'def __init__(self, event, stakeholders, technical_cause, ethical_lapse):', 'def derive_lessons(self):', 'def report(self):', '# Example usage:'] examples)
- Chapter 9: Guardians at the Gates – Compliance and CI/CD (['# compliance_gate.py', 'import sys', 'def check_dependencies():', '# Example rule: Ensure vulnerable libraries are not used', 'def check_for_sensitive_data():', '# Example rule: Prevent hardcoded credentials', 'if "password=" in line.lower():'] examples)
- Chapter 10: Sacred Tools – Frameworks for Redemption (['from aequitas.group import Group', 'from aequitas.bias import Bias', 'from aequitas.fairness import Fairness', '# Load example results dataframe with demographic labels', 'g = Group()', 'xtab, _ = g.get_crosstabs(results)', '# Run bias and fairness analysis', 'b = Bias()', 'f = Fairness()', 'fairness_df = f.get_group_value_fairness(bias_df)', 'from fairlearn.reductions import', 'from sklearn.linear_model import', '# Define fairness constraint', 'constraint = DemographicParity()', '# Train with fairness-aware reduction', 'ExponentiatedGradient(LogisticRegression(), constraints=constraint)', 'preds = estimator.predict(X_test)', 'import shap', '# Train a model', 'model = xgboost.XGBClassifier().fit(X_train, y_train)', '# Explain predictions', 'explainer = shap.TreeExplainer(model)', 'shap_values = explainer.shap_values(X_test)', '# Visualize one decision', 'import lime', 'import lime.lime_tabular', 'import pandas as pd', 'from sklearn.metrics import accuracy_score', '# Example: fairness check across groups', 'def group_accuracy(y_true, y_pred, group_labels):', 'mask = (group_labels == group)', 'acc = accuracy_score(y_true[mask], y_pred[mask])', '# Simulated example', 'y_true = pd.Series([1,0,1,0,1,0])', 'y_pred = pd.Series([1,0,0,0,1,1])', "groups = pd.Series(['A','A','B','B','B','A'])"] examples)
- Chapter 11: The Builders of the New Garden – Ethics in MLOps (['from sklearn.metrics import accuracy_score', 'from fairlearn.metrics import MetricFrame, selection_rate, demographic_parity_difference', 'import numpy as np', '# Example predictions and labels', 'y_true = np.array([1, 0, 1, 1, 0, 0, 1, 0])', 'y_pred = np.array([1, 0, 1, 0, 0, 1, 1, 0])', '# Accuracy overall', '# Fairness metrics', 'import numpy as np', 'import pandas as pd', 'from sklearn.model_selection import', 'from sklearn.linear_model import', 'from sklearn.metrics import accuracy_score', 'from fairlearn.metrics import MetricFrame, selection_rate', '# Synthetic dataset', '"group": np.random.choice(["A", "B"], size=data_size)  # Demographic group', '# Train-test split', 'train_test_split(X[["feature1", "feature2"]], y, test_size=0.3)', '# Model', 'model = LogisticRegression()', 'y_pred = model.predict(X_test)', '# Fairness metrics', 'from fairlearn.reductions import', 'constraints=DemographicParity()', 'y_pred_mitigated = mitigator.predict(X_test)'] examples)
- Chapter 12: Prophecies of Tomorrow – Where Regulation Leads (['class Watchtower:', 'def __init__(self, model, metrics, thresholds):', 'def log_and_validate(self, input_data, output_data):', 'value = func(input_data, output_data)', '# Example usage:', 'predictions = my_model.predict(test_data)', 'import pandas as pd', 'from sklearn.metrics import accuracy_score', 'from sklearn.model_selection import', 'from sklearn.ensemble import', '# Sample dataset with bias risk: gender bias in loan approvals', '# Encode gender', 'X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)', 'model = RandomForestClassifier()', '# Basic accuracy', '# Fairness check: group approval rates', 'from fairlearn.metrics import', 'from sklearn.metrics import accuracy_score', '# y_true = actual outcomes', '# y_pred = model predictions', '# sensitive_features = sensitive attributes like gender, race', 'dpd = demographic_parity_difference(y_true, y_pred, sensitive_features=sensitive_features)', 'European Commission. (2021). Proposal for a regulation laying down harmonised rules on artificial intelligence (Artificial Intelligence Act). https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX%3A52021PC0206', 'Solove, D. J. (2021). The myth of the privacy paradox. George Washington Law Review, 89(1), 1–55. https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3536265'] examples)
